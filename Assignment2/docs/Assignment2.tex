\documentclass[12pt]{article}
\usepackage[a4paper]{geometry}
\usepackage[myheadings]{fullpage}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage[english]{babel}
\usepackage{sectsty}
\usepackage{url, lipsum}
\usepackage{tgbonum}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{datetime}

\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

\pagestyle{fancy}
\fancyhf{} 
\fancyhead[L]{Group 5}
\fancyhead[C]{CP431 - A2}
\fancyhead[R]{02.27.23}
\fancyfoot[C]{\thepage}

\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
% \pgfplotsset{compat=1.9}
% \usepgfplotslibrary{external}
% \tikzexternalize

\lstset{frame=tb,
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\begin{document} {
    % ------------- COVER PAGE -------------
    \fontfamily{put}\selectfont
    \title
    { 
        \normalsize \textsc{}
        \\ [2.0cm]
        \HRule{3pt} \\
        \LARGE \textbf
        {
            {
                CP431 - Parallel Programming \\
                Assignment \#2 \\
            }
            \HRule{3pt} \\ [0.5cm]
            \textbf{\LARGE{Topic: Parallel Merging of Two Sorted Arrays}}
        }
    }
    
    \author {
    	Aidan Traboulay, Memet Rusidovski, \\
        Mobina Tooranisama, Nausher Rao, Zamil Bahri \\ \\
    	Group 5
	}
    
    \date {
        \textbf{\today}
    }
    
    \maketitle
    % ------------- INTRO --------------
    \newpage
    \section {Introduction}
    This report presents a comprehensive evaluation of the performance and runtime of a parallel merge algorithm to merge two arrays efficiently, on large, randomly generated arrays of numbers (up to a billion). The report focuses on the comparison of the runtime and scalability of the implementation and provides insights into it's strengths and weaknesses through benchmarking. Additionally, the report highlights the challenges faced during the parallelization process and discusses potential solutions for improving the performance of the program.

    \noindent The approach used was as follows. Consider two sorted arrays $A$ and $B$, and let $C$ be the merged array. First, split $A$ equally based on the number of processors. Then, split $B$ such that for any groups $A_k$ and $B_l$, $max(A_k) > max(B_l)$. Appropriate breakpoints in $B$ can be found via binary search. Each pair of groups from $A$ and $B$ are then merged linearly by each processor, and then all groups are merged. This strategy ensures that none of the elements in group $C_m$ are greater than elements in $C_{m+1}$.
    
    % ------------- IMPLEMENTATION --------------
    \newpage
    \section {Implementation}
    \subsection{Main Implementation ($main.py$)}
    \begin{lstlisting}[language={Python}]
import numpy as np
import sys
import sort
import time
from mpi4py import MPI
from typing import Tuple

BIG_N = 100_000_000                         # The number of integers
MAX_INT = min(2_147_000_000, 10 * BIG_N)    # The range in which random ints are generated
PRINT_ARRAY_LIMIT = 20                      # Don't print arrays if size exceeds this value
SHOULD_CHECK_CORRECTNESS = False            # Check if the merged array is sorted - quite slow for large values of N
SEED = 50                                   # So that same arrays are generated each run for consistent tests 


def generate_sorted_int_array(lower: int, upper: int, n: int, data_type: str ='i') -> np.ndarray:
    """
    Description:
        Generates a sorted array of random integers.

    Parameters:
        lower: The lower bound of the random integers.
        upper: The upper bound of the random integers.
        n: The number of random integers to generate.
        data_type: The data type of the array. Default is 'i' for int32.

    Returns:
        A sorted array of random integers.
    """
    return np.sort(np.random.randint(lower, upper, size=n, dtype=data_type))


def simple_merge(arr1: np.ndarray, arr2: np.ndarray) -> np.ndarray:
    """
    Description:
        Merges two sorted arrays into one sorted array.

    Parameters:
        arr1: The first sorted array.
        arr2: The second sorted array.

    Returns:
        A sorted array containing all elements of arr1 and arr2.
    """
    return np.sort(np.concatenate((arr1, arr2)), kind='merge')


def binary_search(arr: np.ndarray, elem: any, start: int) -> int:
    """
    Description:
        Performs a binary search on an array for a given element.

    Parameters:
        arr: The array to search.
        elem: The element to search for.
        start: The index to start the search from.

    Returns:
        The index of the element if it is found, or the index of the first
                value greater than it.
    """
    first = start
    last = len(arr)
    mid = (first + last) // 2
    while(first <= last):
        if(mid == len(arr) or arr[mid] == elem):
            return mid

        elif(arr[mid] < elem):
            first = mid + 1

        else:
            last = mid - 1

        mid = (first + last) // 2

    return mid + 1


def check_correctness(a: np.ndarray, b: np.ndarray, merged: np.ndarray) -> bool:
    """
    Description:
        Checks the correctness of the merged array by comparing it to the
        result of a simple merge of the two arrays, and prints the result.

    Parameters:
        a: The first sorted array.
        b: The second sorted array.
        merged: The merged array.

    Returns:
        True if the merged array is correct, False otherwise.
    """
    print("Checking for correctness...")
    is_correct = (merged == simple_merge(a, b)).all()
    print("Correct" if is_correct else "Incorrect")
    return is_correct


def root_process(comm: any, rank: int, size: int) -> None:
    """
    Description:
        The root process of the program. Generates two sorted arrays of random
        integers, then uses parallelisation to merge them into one sorted array.

    Parameters:
        None
    """
    def array_generation() -> Tuple[np.ndarray, np.ndarray]:
        """
        Description:
            Generates two sorted arrays of random integers.

        Parameters:
            None

        Returns:
            A tuple containing the two generated arrays.
        """
        gen_arr_time_start = MPI.Wtime()
        a = generate_sorted_int_array(0, MAX_INT, BIG_N)
        b = generate_sorted_int_array(0, MAX_INT, BIG_N)
        gen_arr_time_end = MPI.Wtime()

        print(f"Time to generate and sort arrays = {gen_arr_time_end - gen_arr_time_start: .4f} seconds.\n")
        print(f"The two generated arrays are of length {BIG_N: ,}.")
        if(BIG_N < PRINT_ARRAY_LIMIT):
            print("The two arrays are:")
            print(f"A: {a}")
            print(f"B: {b}")

        else:
            print("The arrays are too large to print.")

        return a, b

    def parallel_merge(a: np.ndarray, b: np.ndarray, size: int) -> np.ndarray:
        """
        Description:
            Performs a parallel merge of the two arrays.

        Parameters:
            a: The first sorted array.
            b: The second sorted array.
            size: The number of processors

        Returns:
            A sorted array containing all elements of a and b.
        """
        merge_start_time = MPI.Wtime()

        # Split the list into parts based on number of processors
        a_s = np.array_split(a, size - 1)
        for i in range(1, size):
            # tag 0 is an A list block
            comm.send(len(a_s[i-1]), dest=i, tag=3)
            comm.Send([a_s[i-1], MPI.INT], dest=i, tag=0)

        # O(log n) search for break points in B to create blocks
        breaks = [0] * size
        for i, group in enumerate(a_s):
            b_index = sort.binary_search(b, group[-1], breaks[i])
            breaks[i+1] = b_index

        # Set last element to be the length of the array
        breaks[-1] = len(b)
        for i in range(1, size):
            # tag 1 is a B list block
            lower, upper = breaks[i-1], breaks[i]
            comm.send(len(b[lower:upper]), dest=i, tag=4)
            comm.Send([b[lower:upper], MPI.INT], dest=i, tag=1)

        merged = [None] * (size - 1)
        for i in range(1, size):
            # tag 5 is length of merged array, tag 6 is the merged array
            size = comm.recv(source=i, tag=5)
            local_merged = np.empty(size, dtype='i')
            comm.Recv([local_merged, MPI.INT], source=i, tag=6)
            merged[i-1] = local_merged

        merged = np.concatenate(merged, dtype='i')
        merge_end_time = MPI.Wtime()

        print(f'Size of merged array: {sys.getsizeof(merged) / 1_000_000: ,.2f} MB')
        print(f"Time to parallel merge = {merge_end_time - merge_start_time: .4f} seconds")
        if(BIG_N < PRINT_ARRAY_LIMIT):
            print(f"Merged: {merged}")

        return merged

    print(f"Number of processors: {size}")
    a, b = array_generation()
    merged = parallel_merge(a, b, size)
    if(SHOULD_CHECK_CORRECTNESS):
        check_correctness(a, b, merged)


def non_root_process(comm: any, rank: int, size: int) -> None:
    """
    Description:
        A non-root process of the program. Receives two sorted sub-arrays from the
        root process, merges them into one sorted array, and sends it back to the
        root process.

    Parameters:
        None
    """
    # Get an A block, first size then numpy array
    x: int = comm.recv(source=0, tag=3)
    data = np.empty(x, dtype='i')
    comm.Recv([data, MPI.INT], source=0, tag=0)

    # Get a B block of arbitrary size
    y: int = comm.recv(source=0, tag=4)
    datab = np.empty(y, dtype='i')
    comm.Recv([datab, MPI.INT], source=0, tag=1)

    # Merge data and datab
    #merged = simple_merge(data, datab)
    r = MPI.Wtime()
    merged = sort.sort(data, datab, x, y)
    t = MPI.Wtime()

    print(f"Rank {rank} took{t - r: .4} seconds to merge locally")
    # tag 5 is length of merged array, tag 6 is the merged array
    comm.send(merged.size, dest=0, tag=5)
    comm.Send([merged, MPI.INT], dest=0, tag=6)


def main():
    """
    Description:
        The main function of the program. Generates two sorted arrays of random
        integers, then uses parallelisation to merge them into one sorted array.

    Parameters:
        None
    """
    # np.random.seed(round(time.time() * 1000))
    np.random.seed(SEED) # Used this for consistent testing

    # Basic setup for MPI stuff.
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    if rank == 0:
        root_process(comm, rank, size)

    elif rank >= 1:
        non_root_process(comm, rank, size)

    return

if(__name__ == "__main__"):
    main()

    \end{lstlisting}
    
\pagebreak

    \subsection{Utility Function ($cythonizer.py$)}
    \begin{lstlisting}[language={Python}]
from distutils.core import setup
from Cython.Build import cythonize
import numpy

setup(
    ext_modules=cythonize('sort.pyx'),
    include_dirs=[numpy.get_include()]
)
    \end{lstlisting}
    \subsection{Cython Implementation ($sort.pyx$)}
    \begin{lstlisting}[language={Python}]
import cython
from cpython cimport array
import numpy 
cimport numpy

# Sort custom sort function which compiles to C for optimal performance 
ctypedef numpy.ndarray DTYPE_t
@cython.boundscheck(False)
@cython.wraparound(False)
def sort(numpy.ndarray[int, ndim=1] data, numpy.ndarray[int, ndim=1] datab, int x, int y):
    cdef int i = 0
    cdef int j = 0
    cdef int k = 0
    
    cdef numpy.ndarray[int, ndim=1] merged = numpy.ndarray(x+y, dtype='i')
    #cdef array.array merged = array.array('i', [])
    
    while i < x and j < y:
        if data[i] < datab[j]:
            merged[k] = data[i]
            i += 1
        else:
            merged[k] = datab[j]
            j += 1
        k += 1

    return merged

@cython.boundscheck(False) # turn off bounds-checking for entire function
cpdef list split(int size, numpy.ndarray[int, ndim=1] b, a_s):

    cdef list breaks = [0]*size 
    breaks[0] = 0

    cdef int b_index = 0 
    cdef int breaks_index = 1
    cdef int x = len(b)
    cdef int com = 0

    for group in a_s:
        com = group[-1]
        while b_index < x:
            if b[b_index] > com:
                breaks[breaks_index] = b_index
                breaks_index += 1
                break
            b_index += 1
    
    breaks[-1] = len(b)

    return breaks

@cython.boundscheck(False)
@cython.wraparound(False)
cpdef int binary_search(numpy.ndarray[int, ndim=1] arr, int elem, int start):
    cdef int first = start
    cdef int last = len(arr)
    cdef int mid = int((first + last)/2)
    
    while first <= last:
        if (mid == len(arr) or arr[mid] == elem):
            return mid
        elif arr[mid] < elem:
            first = mid + 1
        else:
            last = mid - 1

        mid = int((first + last)/2)

    return mid + 1

def test(x):
    cdef int y = 5

    return x + y 
    \end{lstlisting}
    
     % ------------- BENCHMARKS --------------
    \newpage
    \section {Benchmarks}

\subsection {System Information}
These benchmarks were run on the Teach SciNet cluster. The cluster consists of 42 repurposed x86\_64 nodes each with 16 cores (from two octal core Intel XeonSandybridge E5-2650 CPUs) running at 2.0GHz with 64GB of RAM per node. For the following tests, the number of tasks per node was maximized as to reduce communication overhead as much as possible. For example, if the test was with 32 cores, a configuration of 2 nodes with 16 tasks per node would be used, as opposed to 4 nodes with 8 tasks per node. The exception was the test with 24 nodes, where the configuration was 3 x 8. Furthermore, for $p < 8$, values like $p = 2^n + 1$ were preferred, because $p-1$ cores were actually utilizing parallelization.

\subsection{Graphs}

The following graphs are on a log-log axis. The blue line represents the average local merge time per processor, whereas the red line represents the total time in took for the entire merge process. This does \emph{not} include array generation, but it does include splitting the arrays, sending and receiving data to and from other processors, and the final merging step.

\subsubsection{N = 1,000,000,000}

\pgfplotstableread[col sep = comma]{benchmarks/bench_n1B.csv}\mydata
\begin{tikzpicture}
   
\begin{loglogaxis}[
    log ticks with fixed point,
    log basis x=2,
    log basis y=2,
    legend pos = south west,
    tick label style = {font=\fontsize{7}{7}},
    grid=both,
    tick align = outside,
    tickpos = left,
    xmax = 128,
    ymax = 64,
    xlabel = {Number of Processors},
    ylabel = {Time$(s)$},
    table/x index={0},
    width = 0.9\textwidth,
    height = 0.6\textwidth
]
\addplot [mark = *, color=red] table[header=false, y index={1}]{\mydata};
\addlegendentry{Total Time for Parallel Merge}
% \addplot [color=red] table[y index={1}]{\mydata};
\addplot [only marks, color=blue] table[header=false, y index={2}]{\mydata};
%  Linear Regression Line
\addplot[blue,thick] table[y={create col/linear regression={y=Y}}]
{
    X	Y
    2	20.81515207
    3	10.72192
    5	5.13798
    8	3.080189
    16	1.4665
    24	0.9684
    32	0.7391
    48  0.4521
    64	0.3228
};
\addlegendentry{Average Merge Time Per Processor}
\end{loglogaxis}
\end{tikzpicture}

\subsubsection{N = 100,000,000}
\pgfplotstableread[col sep = comma]{benchmarks/bench_n100M.csv}\mydata
\begin{tikzpicture}
\begin{loglogaxis}[
    log ticks with fixed point,
    log basis x=2,
    log basis y=2,
    legend pos = south west,
    tick label style = {font=\fontsize{7}{7}},
    grid=both,
    tick align = outside,
    tickpos = left,
    xmax = 128,
    ymax = 4,
    xlabel = {Number of Processors},
    ylabel = {Time$(s)$},
    table/x index={0},
    width = 0.9\textwidth,
    height = 0.6\textwidth
]
\addplot [mark = *, color=red] table[header=false, y index={1}]{\mydata};
\addlegendentry{Total Time for Parallel Merge}
\addplot [only marks, color=blue] table[header=false, y index={2}]{\mydata};
%  Linear Regression Line
\addplot[blue,thick] table[y={create col/linear regression={y=Y}}]
{
    X	Y
    2	1.9132
    3	1.0198
    5	0.5297
    8	0.3087
    16	0.1449
    24	0.0945
    32	0.0701
    48  0.0414
    64	0.0346

};
\addlegendentry{Average Merge Time Per Processor}
\end{loglogaxis}
\end{tikzpicture}

\subsubsection{N = 10,000,000}
\pgfplotstableread[col sep = comma]{benchmarks/bench_n10M.csv}\mydata
\begin{tikzpicture}
\begin{loglogaxis}[
    log ticks with fixed point,
    log basis x=2,
    log basis y=2,
    legend pos = south west,
    tick label style = {font=\fontsize{7}{7}},
    grid=both,
    tick align = outside,
    tickpos = left,
    xmax = 128,
    ymax = 1,
    xlabel = {Number of Processors},
    ylabel = {Time$(s)$},
    table/x index={0},
    width = 0.9\textwidth,
    height = 0.6\textwidth
]
% Red Line
\addplot [mark = *, color=red] table[header=false, y index={1}]{\mydata};

\addlegendentry{Total Time for Parallel Merge}
% Blue Line
\addplot [only marks, color=blue] table[header=false, y index={2}]{\mydata};
%  Linear Regression Line
\addplot[blue,thick] table[y={create col/linear regression={y=Y}}]
{
    X   Y
    2   0.2033
    3   0.1032
    5   0.0517
    8   0.031148
    16  0.01524
    24  0.01015
    32  0.007729
    48  0.005083
    64  0.003656
};
\addlegendentry{Average Merge Time Per Processor}
\end{loglogaxis}
\end{tikzpicture}
    
    % ------------- ANALYSIS --------------
    \newpage 
    \section {Analysis}
    \subsection{Code Analysis}
    One thing to note is that in this implementation, given $P$ processors, parallelization only happens in $(P-1)$ processors, since $p_0$ is the root node and responsible for generating the two arrays, and sending and receiving data. The array generation is the longest part of the total runtime of the program. For instance, on the teach cluster, it would take upto $~275$ seconds to generate both arrays for $n = 10^9$. File I/O was also tested, but importing data from a file took longer than generating and sorting. This could have been improved if two processors were used to generate the arrays instead, but it was not relevant to the parallel merge algorithm.

    The $B$ array was split in the way mentioned earlier. In an earlier version of the program, the breakpoints were searched for linearly. However, when a binary search algorithm was implemented, even on a single core, over 20x improvement was measured for large values $n$. It's worth noting that since the breakpoints would be calculated serially, each subsequent binary search took place over a smaller search space. A parallel implementation of this was attempted, where each processor would be responsible for finding it's own range in $B$. However, it was consistently slower than the single core implementation, and it got even slower for increasing core counts. The reason was that sending the entirety of $B$ to other processors had significant overhead. Therefore, the parallel implementation was rejected.

    Once the breakpoints were found and the arrays had been split, the optimizing the local merge process was the next challenge. Two methods were attempted: (1) a simple linear merge (compare $A_i$ and $B_j$, and append the smaller to the $C_k$), and (2) using numpy's $concatenate()$ and $sort()$ functions. Since numpy is written in C, it turned out be faster than the linear merge. However, using a linear merge felt more appropriate in the context of the problem. Implementing the linear merge in Cython resulted in the time to be very close to numpy's version.

    Once the local merges were calculated, they were sent back to the root process, where the fully merged array was formed by concatenating all the local merges. 

    \subsection{Benchmark Analysis}
    Looking at the graphs, it is immediately obvious that there exists a sweet spot in the processor count, and the total merge time starts increasing past the sweet spot. This is in contrast to the local merge times, where they would consistently decrease for increasing processor counts. It turns out that the merging process itself accounted for a very small percentage of the total parallel merge times. Most of the runtime came from inter-processor communications, i.e., sending and receiving the sub arrays from and into the root process. For smaller $n$, the runtime of the overhead associated with a large processor count takes over any useful operations. For larger $n$, it takes a larger amount of processors for the overhead to become significant, but it still exists. This is why for $n = 10^7$, the optimal processor count seems to be 8, whereas for $n = 10^8$ and $n = 10^9$, it seems to be 16. It would be a reasonable guess that for $n = 10^{10}$, the optimal would be 24.

    \subsection{Further Improvements}
    % Generate arrays separately in two different ranks. Reduces array generation time significantly
    %   - A is generated in p0, split, and A's subarray is sent to p1
    %   - p1 generates B, waits for A subarray splits, then searches for breakpoints without ever needing to
    %       communicate with p0 about B.
    \subsubsection{Array Generation}
    Instead of generating both arrays within the same rank, $p_0$ could be used to generate $A$, split it, and then send two sets of data: (1) the last values (which are the maximum values) of all subarrays to $p_1$, and (2) the subarrays themselves to their associated ranks. In the meantime, $p_1$ would generate $B$, receive the max values from $p_1$ to find breakpoints in $B$, and then send these breakpoints to their associated ranks. Note that while there might be \emph{some} waiting involved, the array generation itself will be happening simultaneously, therefore reducing the overall runtime significantly.

    \subsubsection{Breakpoint Searching Parallelization}
    Ideally, each rank should be responsible for finding it's own breakpoint in $B$. However, since the rank does know the lower search bound yet, it would need to search through the entire $B$ array. The issue here is not the searching itself, but rather \emph{sending} $B$ to the other ranks. During testing, the process of sending $B$ to other ranks negated any parallelization improvements. One simple solution would be to implement collective communication; since the same data is being sent to all processors, multiple processors can also partake in sending the data once they receive it. 

    \noindent Another solution is to use shared memory within each node, and only send $B$ to another rank if they belong to a different node. Consider the Teach cluster: where the number of nodes available for access was 4, $B$ would only need to be sent 4 times, instead of 64 times. For an even larger number of nodes, this idea can be used with collective communication for even faster communication. This can implemented by using a combination of MPI and OpenMP.

    \subsubsection{Final Merging Process}
    Instead of using consecutively concatenating the local merges, the merges can occur in pairs across the processors. This strategy would be like an upside down binary tree, where in each stage, pairs of processors would combine their locally merged arrays into a larger merged array. At each stage, the merges would be happening parallel, and so for $P$ processors, it would take $log(P)$ steps to get the final merged array, as opposed to $P$ steps in the current implementation.  
    
    % ------------- REFERENCES --------------
    \newpage
    \begin{thebibliography}{5}
    \bibitem{MyLS} "L4\_Parallel\_Benchmarking." MyLearningSpace. (n.d.). Retrieved January 30, 2023, \emph{https://mylearningspace.wlu.ca/d2l/le/content/471549/viewContent/3194181/View}
    \bibitem{MyLS}“Parallel\_Merging\_Algorithm.” MyLearningSpace. (n.d.). Retrieved February 22, 2023, \emph{https://mylearningspace.wlu.ca/d2l/le/content/471549/viewContent/3194203/View}
    \bibitem{Teach} "Teach SciNet Documentation." Teach. (28 January 2023). Retrieved February 28, 2023, \emph{https://docs.scinet.utoronto.ca/index.php/Teach}
    \end{thebibliography}
    
\end{document}